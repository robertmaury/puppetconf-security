Hey. My name is Robert Maury, a Technical Solutions Engineer at Puppet Labs. I like start off with a bit of my background so that you can get a feeling whether you want to listen to what I say at all. I'm a relatively recent arrival at Puppet, having come from Oracle's Public Cloud group. There, I was involved with onboarding recent acquisitions, integrating their existing systems with Oracle's. This involved a great deal of automation, as well as shimming in security standards where none may have existed before.

To give you an idea of what this looked like in practice, we were working against a "go live" date at the end of the month. Two weeks before hand, we were notified that we were going to have to pass a security audit. This involved a lot of scrambling around and Robert being notified that this was now his 100% project.

Now Oracle has a very robust security apparatus. They published extensive documentation as to which security standards we were expected to conform to. Even better, they published the script they would be using to audit our production environments. Those of you have migrated an existing infrastructure to Puppet are probably well versed in converting scripts into the Puppet DSL. After 3-4 15 hour days, we passed our audit with flying colors and kept Larry happy.

Puppet happens to be very well positioned to address an organization's security needs through state based node management, as well as its robust reporting mechanisms. The ability to guarantee the integrity of your environment is often balanced by concerned that there is now a single point of "failure" that, if exploited, offers your entire infrastructure on a silver platter.  

First, let's talk a bit about the security of Puppet itself. Puppet is built off a strong master/agent model. The agent itself treats the master as a black box. All it knows to do is submit its Facts and expect back a catalog. The agent has very little control over it's own destiny, and we want to keep it that way.

What does this get you in practice? It prevents a malicious actor from exploiting a less critical node and using it access otherwise protected data. Imagine you have a node that you have set up to allow customers to FTP data to you. If you offload more management functions to the agent, i.e., allow a node to determine its own classification, that malicious actor now has the ability add an application server role. Now you're in danger of exposing your source code, or even database connection strings and passwords.

Part of Puppet's security model is institutional rather than technical. From http://puppetlabs.com/security, "Puppet Labs supports responsible disclosure of security vulnerabilities." Here, you can find a list of outstanding security vulnerabilities. As they are patched, they are moved off the list and noted in the release.

Beyond the Puppet Master itself, the primary concern should be securing both your module pipeline as well as whatever mechanism you use to abstract your organizational data. There is a wealth of information available online on workflows for Puppet code promotion. Because you're treating your infrastructure as code, this enables some very robust workflows that we have borrowed from software engineering disciplines. At Oracle, we used both human and automated testing of all code changes. Using GIT, all engineers had the ability to create pull requests against the module repository. We had a policy that all changes had to be approved by two peers before it could be merged into the master. In more sensitive environments, you may want to have a colleague in a security role have final oversight over what code gets merged. 

Once a change has been approved, it then went through automated testing. In our workflow, we used to Jenkins to pull all code changes and ran, at minimum, simple syntax and convergence tests. Using Puppet LINT, you can test to make sure that any changes are written using best practices. Puppet Spec allows you to create unit and regression tests to make sure you haven't breaking changes. Finally, with Puppet Beaker, you can created a robust suite of integration tests to confirm that your code performs as expected. Only then did changes get promoted up to the Master.

An interesting idea that recently came up is the question, "Can you write Unit and Integration tests so that, if a module passes them, it guarantees compliance with X security standard." 

As an aside, I often get asked questions around change management. Puppet's simulation mode is a pretty powerful tool, and sometimes organizations seize on the idea of running in simulation mode throughout the day until they hit some predetermined change window. This undermines Puppet's ability to remediate unexpected changes. What I'd recommend is to leverage these robust code management workflows to mitigate the risks that come with change. Remember, Puppet will enforce no NEW changes until the code has been promoted to the Master. With this in mind, you can leverage Jenkins in a Continuous Integration environment, only promoting the code to the Master during a pre-arranged change window.

Now that we're comfortable with managing the Master and our associated code, let's take a look at the control this affords us. I'm sure most of have seen the power in being able to build a model and applying it consistently across your enterprise. How many of you actually manage iptables rules at the node level? For those of you who don't, why don't you? I would argue that it's a pain. You can't just ensure that your config is consistent across your environment, you have to account for every application, middleware, and service you're running. A well designed Puppet architecture lets you take a very application specific view of your environment. 

Here's an example of a secure firewall baseline we can apply to our environment. We're going to accept ping, loopback connections, and established connections. Otherwise, shut everything down. You notice I didn't even up a port for ssh. I might argue that, in a mature Puppet environment, you would block ssh to ensure all changes are forced through our change management workflow, but that's a lot to ask. Let's take a look at our openssh module. If we're taking an application level view, let's bundle all our configuration items here. In this case, I'm going to go ahead and open up port 22 to enable ssh traffic.

 This is a fairly basic example, but you can see how easy it was to compartmentalize even complex configuration challenges. It's beyond the scope of this talk (and it gives me a headache), but you can also use this pattern to streamline management of your selinux configs.

Now, in my opinion, one of the greatest strengths of Puppet is the fact that, if you have a problem, somebody has probably already solved it for you. If you're lucky, this may also be the case as you try to build a secure baseline config. There are a number of different security standards that you might be subject to. PCI, SOX [name some other], but let's take a look at the STIG standards. For those of you who are not familiar, STIG, or the Security Technical Implementation Guides, are published by the Department of Defense, and are available for both Linux and Windows systems. There is a version for classified systems that I can't get my hands on, but they do publish guides for the civilian space as well. This is probably a good start for organizations that are not obligated to follow a specific standard, but do want to conform to a published baseline.

The primary reason I wanted to take a look at the STIG standards and Puppet is that, again, most of the work has already been done for me. Within Fedora, there is a group who work on what they call the Aqueduct project. In their words, "Aqueduct aims to simplify compliance of Red Hat Enterprise Linux through the sharing of information, ideas, techniques, and best practices of system configuration content..." What I was most interested in was the fact that they released a STIG baseline built around Puppet. 

Now the modules are a bit dated so I massaged them a bit and you should find a set of updated modules on my github page. Now what you see here is a very service based approach to security, and if your end goal compliance, this is probably the approach I would take. They have broken the standard down into functional modules which you can incorporate into your internal standards. Earlier, we discussed how we might better secure the Puppet Master, and I would start right here. 

One caveat, by default Aqueduct includes an auditd module with a fairly comprehensive ruleset. This is not a bad thing; however, auditd is chatty so you'll probably want to move its logs to its own partition and rotate aggressively. I see auditd as a complimentary tool to Puppet's built in reporting. Let's take a look at an example of how auditing works within Puppet. Part of our secure baseline manages /etc/ssh/sshd.conf and disables root login. Let's pretend I'm Joe "I hate using sudo" and flip that back. Now we know that Puppet is going to revert that back at the next Puppet run. Looking at our Event Viewer, this gives us a lot of information about what's going on here. Not only does it tell us that is remediated an out of band change, it also tells us why. With the manifest path and line number, we can take a look at exactly why Puppet is enforcing this config. If we have written our modules with this in mind, we can comment our resources with the particular standard we're enforcing. 

I was once speaking to a customer who made the comment, "It doesn't matter how secure my infrastructure is, if I can't prove it to my auditors." What some organization's will do is teach their auditors how to read their modules. Good Puppet code should be very human readable. At this point, it's just a matter of giving them access to reporting console which lays out in black and white (green and blue) what nodes are in compliance, and what needs to be further examined.

So, in summary Puppet provides a robust toolset for managing security baselines and reporting against them. 

